{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI SVD\n",
    "-------\n",
    "\n",
    "Testing out the idea that useful word embeddings can be gleaned by performing the SVD on the matrix of pointwise mutual information between words and their co-occurrances. [Idea from here.](http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD as SVD\n",
    "import collections\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', stream=sys.stdout, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "----\n",
    "\n",
    "For this exercise I used MF DOOM song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "\n",
    "def process_file(directory, f):\n",
    "    content = open(os.path.join(directory, f), \"r\").read()\n",
    "    body = re.split('\\n{2,}', content)\n",
    "    return body[1].lower().split(\"\\n\")\n",
    "\n",
    "def text_data(directory):\n",
    "    all_files = reduce(lambda x,y: x+y, [files for sub, dirs, files in os.walk(directory)], [])\n",
    "    text = [(f, process_file(directory, f)) for f in all_files]\n",
    "    return dict(text)\n",
    "\n",
    "file_text = text_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(text_dict):\n",
    "    words = [word for file, lines in text_dict.iteritems() for line in lines for word in line.split()]\n",
    "    return collections.Counter(words)\n",
    "\n",
    "def skip_grams(line):\n",
    "    return [tuple(sorted(list(t))) for t in itertools.combinations(line.split(), 2)]\n",
    "\n",
    "def skip_gram_counts(text_dict):\n",
    "    grams = [gram for file, lines in text_dict.iteritems() for line in lines for gram in skip_grams(line)]\n",
    "    return collections.Counter(grams)\n",
    "\n",
    "\n",
    "word_counter = word_counts(file_text)\n",
    "gram_counter = skip_gram_counts(file_text)\n",
    "\n",
    "word_index = dict([(word, index) for index, word in enumerate(word_counter.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   is there a nice functional way to do this in python?\n",
    "\"\"\"\n",
    "word_count = len(word_index)\n",
    "total_words = float(sum(word_counter.values()))\n",
    "total_grams = float(sum(gram_counter.values()))\n",
    "\n",
    "pmi_matrix = np.zeros((word_count, word_count))\n",
    "\n",
    "for word1, row in word_index.iteritems():\n",
    "    for word2, column in word_index.iteritems():\n",
    "        tup = tuple(sorted([word1, word2]))\n",
    "        \n",
    "        gram_pct = gram_counter[tup]/total_grams\n",
    "        word1_pct = word_counter[word1]/total_words\n",
    "        word2_pct = word_counter[word2]/total_words\n",
    "        \n",
    "        pmi = 0 if gram_pct == 0 else np.log(gram_pct/(word1_pct*word2_pct))\n",
    "        pmi_matrix[row, column] = pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_svd = SVD(n_components=50).fit(pmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   obviously this could be done more efficiently\n",
    "\"\"\"\n",
    "def similar_words(word, word_index, svd, to_get=10):\n",
    "    if word not in word_index:\n",
    "        return []\n",
    "    else:\n",
    "        index = word_index[word]\n",
    "        vectors = svd.components_.T\n",
    "        vector = vectors[index]\n",
    "        \n",
    "        index_to_word = dict([(i,w) for w,i in word_index.iteritems()])\n",
    "        \n",
    "        sims = [(index_to_word[index2], vector.dot(vector2)) for index2, vector2 in enumerate(vectors)]\n",
    "    \n",
    "        return sorted(sims, cmp=lambda x,y: cmp(x[1], y[1]), reverse=True)[:to_get]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 0.0092093790167196501),\n",
       " ('just', 0.0078288557941133749),\n",
       " ('a', 0.0069691862547100166),\n",
       " ('hit', 0.0039355643370884881),\n",
       " ('all', 0.0038155953455745071),\n",
       " ('the', 0.003211757047555244),\n",
       " (\"i'm\", 0.0031908668861541467),\n",
       " ('time', 0.003039398076669351),\n",
       " ('happened', 0.002948875574251482),\n",
       " ('side', 0.0028487890991962597)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words(\"zygote\", word_index, pmi_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
